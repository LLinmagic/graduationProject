
"""
from transformers import BertTokenizer, BertForSequenceClassification


model_name = "bert-base-chinese"
save_dir = r"D:\æ¯•è®¾\ä»£ç \models\bert-base-chinese"

tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name)

tokenizer.save_pretrained(save_dir)
model.save_pretrained(save_dir)

print("bert-base-chinese å·²å®Œæ•´ä¿å­˜åˆ°æœ¬åœ°")
"""
"""
from transformers import BertTokenizer, BertForSequenceClassification
MODEL_PATH = r"D:\æ¯•è®¾\ä»£ç \models\bert-base-chinese"
print("å¼€å§‹åŠ è½½ tokenizer...")
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
print("tokenizer åŠ è½½æˆåŠŸ")

print("å¼€å§‹åŠ è½½ model...")
model = BertForSequenceClassification.from_pretrained(
    MODEL_PATH,
    num_labels=3
)
print("model åŠ è½½æˆåŠŸ")

print("ğŸ‰ æœ¬åœ° BERT è‡ªæ£€é€šè¿‡")

"""



"""from transformers import BertTokenizer, BertForSequenceClassification

BASE_MODEL_PATH = r"D:\æ¯•è®¾\ä»£ç \models\bert-base-chinese"

# ä¸‹è½½ tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
tokenizer.save_pretrained(BASE_MODEL_PATH)

# ä¸‹è½½æ¨¡å‹ä¸»ä½“ï¼ˆä¸å¸¦åˆ†ç±»å¤´ï¼‰
model = BertForSequenceClassification.from_pretrained(
    "bert-base-chinese",
    num_labels=3,                  # ä¸‰åˆ†ç±»
    ignore_mismatched_sizes=True   # è‡ªåŠ¨åˆå§‹åŒ–åˆ†ç±»å¤´
)
model.save_pretrained(BASE_MODEL_PATH)"""



"""
import torch

print(torch.cuda.is_available())  # True è¡¨ç¤ºå¯ä»¥ä½¿ç”¨ GPU
print(torch.cuda.device_count())  # GPU æ•°é‡
print(torch.cuda.get_device_name(0))  # GPU åç§°
"""


"""
2ï¸âƒ£ å¦‚ä½•é€‰æ‹© MAX_LEN
åŸåˆ™ï¼šå°½é‡è¦†ç›–æ–‡æœ¬å¤§éƒ¨åˆ†é•¿åº¦ï¼ŒåŒæ—¶ä¸è¦æµªè´¹è®¡ç®—
æ–¹æ³•ï¼š
ç»Ÿè®¡æ–‡æœ¬é•¿åº¦åˆ†å¸ƒï¼ˆtoken æ•°ï¼‰ï¼Œä¾‹å¦‚å– 95% çš„æ–‡æœ¬é•¿åº¦ä½œä¸º MAX_LEN
CPU è®­ç»ƒæ—¶ï¼Œå¯ä»¥é€‚å½“é™ä½ï¼Œç‰ºç‰²ä¸€äº› padding ç²¾åº¦ï¼Œå‡è½»è´Ÿæ‹…
ä¸¾ä¾‹ Python ç»Ÿè®¡ token é•¿åº¦ï¼š
ç»Ÿè®¡åï¼Œæ¯”å¦‚ 95% æ–‡æœ¬é•¿åº¦ â‰¤ 64ï¼Œå°±å¯ä»¥å®‰å…¨è®¾ç½®ï¼š
MAX_LEN = 64  # CPUæ¨¡å¼ä¸‹å®‰å…¨
å¦‚æœ GPU è¶³å¤Ÿï¼Œå¯ä»¥ç”¨ï¼š
MAX_LEN = 128  # è¦†ç›–å‡ ä¹æ‰€æœ‰æ–‡æœ¬
"""
from transformers import BertTokenizer
import pandas as pd
df = pd.read_csv("å¤æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®_5000.csv")
texts = df["Original Sentence"].dropna().tolist()
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

lengths = [len(tokenizer.tokenize(t)) for t in texts]
lengths.sort()
max_len_95 = lengths[int(len(lengths)*0.95)]
print("95%æ–‡æœ¬é•¿åº¦åˆ†ä½æ•°:", max_len_95)
#95%æ–‡æœ¬é•¿åº¦åˆ†ä½æ•°: 41
"""
CPU æ¨¡å¼
MAX_LEN = 48ï¼ˆç•¥å¤§äº 95% åˆ†ä½æ•°ï¼Œä¿è¯å¤§éƒ¨åˆ†æ–‡æœ¬è¦†ç›–ï¼ŒåŒæ—¶é¿å…è¿‡å¤š paddingï¼‰
BATCH_SIZE = 8ï¼ˆCPU å®‰å…¨ï¼‰
NUM_WORKERS = 1
USE_AMP = False

GPU æ¨¡å¼
MAX_LEN = 64~128ï¼ˆå¯ä»¥è¦†ç›–å‡ ä¹æ‰€æœ‰æ–‡æœ¬ï¼‰
BATCH_SIZE = 16
NUM_WORKERS = 4
USE_AMP = True
è¿™é‡Œ CPU æ¨¡å¼é€‰æ‹© 48ï¼Œæ¯” 41 ç¨å¤§ä¸€äº›ï¼Œç•™å‡ºä¸€å®šä½™é‡ï¼Œè®­ç»ƒè®¡ç®—é‡ä½ï¼ŒCPU æ¸©åº¦æ›´å®‰å…¨ã€‚
"""
