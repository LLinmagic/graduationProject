# train_sentiment.py
# -------------------------------------------------
# æœ¬æ–‡ä»¶ç”¨äºè®­ç»ƒå¤æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆBERT ä¸‰åˆ†ç±»ï¼‰
# åŠŸèƒ½ï¼š
# 1. ä½¿ç”¨ PyTorch AdamW è®­ç»ƒ BERT æ¨¡å‹
# 2. ä¿å­˜æ¨¡å‹å’Œ tokenizer
# 3. å¯é€‰ï¼šè®°å½•è®­ç»ƒæ—¥å¿—åˆ° CSV æ–‡ä»¶
# 4. è‡ªåŠ¨å¤„ç† CSV ä¸­ç¼ºå¤±å€¼å’Œæ ‡ç­¾ç±»å‹
# 5. ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼Œå±è”½ HuggingFace è­¦å‘Š
# -------------------------------------------------


"""è¯¥ç‰ˆæœ¬
æ ¹æ®ç‰ˆæœ¬2ä¸­ï¼Œç±»åˆ«2æƒé‡åå¤§ï¼Œè¿›ä¸€æ­¥å¯¹ç±»åˆ«æƒé‡è¿›è¡Œè°ƒèŠ‚å®éªŒï¼Œæƒé‡å– 2.5 ã€‚
ä½†æ¨¡å‹åœ¨å°‘æ•°ç±»ä¸Šä¸èƒ½è¾¾åˆ°æ›´ä¼˜å¹³è¡¡
ä½†ä¹Ÿä¸å†ç»§ç»­æ”¹æƒé‡ä¸ºï¼š2 / 1.5ï¼Œæ¥è·‘ä»£ç 
å› ä¸ºç¡®å®šï¼š2 / 1.5 ä¸ä¼šâ€œæ›´å¥½â€ï¼Œåªä¼šâ€œçœ‹èµ·æ¥å¯èƒ½æœ‰ç‚¹ä¸ä¸€æ ·â€ã€‚
ä¸‹é¢æ˜¯ä¸ºä»€ä¹ˆå¯ä»¥ç¡®å®šï¼Œä»¥åŠåœ¨ä»€ä¹ˆæç«¯æƒ…å†µä¸‹æ‰å€¼å¾—å†è¯•ã€‚
ä¸€ã€ä¸ºä»€ä¹ˆæˆ‘å¯ä»¥â€œç¡®å®šâ€â€”â€”ä¸æ˜¯å‡­ç»éªŒï¼Œæ˜¯å‡­ç»“æœ
åªç”¨å·²ç»è·‘å‡ºæ¥çš„çœŸå®æ•°æ®ï¼Œä¸åšä»»ä½•å‡è®¾ã€‚
1ï¸âƒ£ æŠŠä¸‰æ¬¡å®éªŒæŒ‰â€œæƒé‡è¿ç»­å˜åŒ–â€æ’æˆä¸€æ¡çº¿
æƒé‡	         ç±»åˆ«2_F1	ç±»åˆ«1_Recall	  Accuracy
1.0 (v1.1)	0.4604	      0.6877	  0.6496
2.5 (v3)	0.4630	      0.5469 â†“	  0.6416
3.0 (v2)	0.4380 â†“	  0.7202	  0.6537

å…³é”®äº‹å®ï¼š
F1 åœ¨ 0.46 å·¦å³éœ‡è¡
æ²¡æœ‰éšæƒé‡å•è°ƒä¸Šå‡
æ¯ä¸€æ¬¡æå‡ï¼Œéƒ½ä¼´éšåˆ«çš„ç±»åˆ«æ˜æ˜¾ä¸‹é™
è¿™å·²ç»è¯´æ˜ï¼š
æ¨¡å‹çš„ç“¶é¢ˆä¸åœ¨ loss æƒé‡ï¼Œè€Œåœ¨â€œè¯­ä¹‰å¯åˆ†æ€§â€ã€‚

æ³¨ï¼š
ç±»åˆ«æƒé‡æœ¬è´¨ä¸Šåšäº†ä¸€ä»¶äº‹ï¼šæ‹‰åŠ¨å†³ç­–è¾¹ç•Œå‘å°‘æ•°ç±»æ–¹å‘å¹³ç§»
ä½†ä½ è¿™ä¸ªæ•°æ®æœ‰ä¸¤ä¸ªç‰¹å¾ï¼š
ç±»åˆ« 2 æ–‡æœ¬çŸ­ã€æ¨¡ç³Šã€è¯­å¢ƒä¾èµ–å¼º
ä¸ç±»åˆ« 0 / 1 åœ¨ embedding ç©ºé—´é«˜åº¦é‡å 

ç»“æœå°±æ˜¯ï¼š
æƒé‡ â†‘ â†’ è¾¹ç•Œæ¨è¿‡å» â†’ è¯¯ä¼¤å¤§é‡ 0 / 1
æƒé‡ â†“ â†’ å›åˆ°åŸè¾¹ç•Œ â†’ è¡Œä¸º â‰ˆ v1
ğŸ“Œ æ²¡æœ‰æ–°çš„ä¿¡æ¯æ³¨å…¥ï¼Œè¾¹ç•Œåªèƒ½å¹³ç§»ï¼Œä¸èƒ½å˜å½¢ã€‚
"""
import os
import csv
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm
import config

# -------------------------------------------------
# è®­ç»ƒæ—¥å¿—å¼€å…³
# -------------------------------------------------
SAVE_TRAIN_LOG = False

# -------------------------------------------------
# è¦†ç›–ä¿å­˜è·¯å¾„
# -------------------------------------------------
config.MODEL_SAVE_PATH = os.path.join("result_model", "sentiment_bert_model_v3")


# -------------------------------------------------
# å±è”½ HuggingFace è­¦å‘Šä¿¡æ¯
# -------------------------------------------------
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "true"
os.environ["HF_HUB_DISABLE_XET_WARNING"] = "true"


# -------------------------------------------------
# 1. è‡ªå®šä¹‰æ•°æ®é›†ç±»
# -------------------------------------------------
class SentimentDataset(Dataset):
    """è‡ªå®šä¹‰æƒ…æ„Ÿåˆ†ææ•°æ®é›†"""

    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }


# -------------------------------------------------
# 2. è¯»å–æ•°æ®å¹¶åˆ’åˆ†è®­ç»ƒ / éªŒè¯é›†
# -------------------------------------------------
def load_data():
    df = pd.read_csv(config.SENTIMENT_DATA_PATH)
    df = df[["Original Sentence", "Sentiment"]]

    print("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
    print(df.isna().sum())

    df = df.dropna(subset=["Original Sentence", "Sentiment"])

    df["Sentiment"] = pd.to_numeric(df["Sentiment"], errors="coerce")
    df = df.dropna(subset=["Sentiment"])
    df["Sentiment"] = df["Sentiment"].astype(int)

    df.columns = ["text", "label"]

    return train_test_split(
        df["text"].tolist(),
        df["label"].tolist(),
        test_size=0.2,
        random_state=config.RANDOM_SEED,
        stratify=df["label"]
    )


# -------------------------------------------------
# 3. å•è½®è®­ç»ƒå‡½æ•°ï¼ˆâ˜…åŠ å…¥ç±»åˆ«æƒé‡ï¼‰
# -------------------------------------------------
def train_model(model, train_loader, optimizer, device, use_amp, scaler, epoch, loss_fn):
    model.train()
    total_loss = 0.0

    if use_amp and scaler is None:
        scaler = torch.amp.GradScaler()

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}", leave=False):
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        if use_amp:
            with torch.amp.autocast(device_type="cuda"):
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                # â˜… ä½¿ç”¨å¸¦ç±»åˆ«æƒé‡çš„ loss
                loss = loss_fn(outputs.logits, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            # â˜… ä½¿ç”¨å¸¦ç±»åˆ«æƒé‡çš„ loss
            loss = loss_fn(outputs.logits, labels)

            loss.backward()
            optimizer.step()

        total_loss += loss.item()

    torch.cuda.empty_cache()
    return total_loss / len(train_loader)


# -------------------------------------------------
# 4. æ¨¡å‹è¯„ä¼°å‡½æ•°
# -------------------------------------------------
def evaluate_model(model, val_loader, device):
    model.eval()
    preds, true_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            predictions = torch.argmax(outputs.logits, dim=1)

            preds.extend(predictions.cpu().numpy())
            true_labels.extend(batch["labels"].numpy())

    print(classification_report(true_labels, preds, digits=4))


# -------------------------------------------------
# 5. ä¿å­˜è®­ç»ƒæ—¥å¿—
# -------------------------------------------------
def save_training_log(log_path, epoch_losses):
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    with open(log_path, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["epoch", "avg_loss"])
        for epoch, loss in enumerate(epoch_losses, start=1):
            writer.writerow([epoch, loss])

    print(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ° {log_path}")


# -------------------------------------------------
# 6. ä¸»ç¨‹åºå…¥å£
# -------------------------------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_amp = device.type == "cuda"
    scaler = torch.amp.GradScaler() if use_amp else None

    train_texts, val_texts, train_labels, val_labels = load_data()

    LOCAL_MODEL_PATH = r"D:\æ¯•è®¾\ä»£ç \models\bert-base-chinese"

    required_files = ["config.json", "vocab.txt", "tokenizer_config.json"]
    model_file = "model.safetensors"

    if os.path.exists(LOCAL_MODEL_PATH) and \
            all(os.path.isfile(os.path.join(LOCAL_MODEL_PATH, f)) for f in required_files) and \
            os.path.isfile(os.path.join(LOCAL_MODEL_PATH, model_file)):
        print(f"ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼š{LOCAL_MODEL_PATH}")
        tokenizer = BertTokenizer.from_pretrained(LOCAL_MODEL_PATH)
        model = BertForSequenceClassification.from_pretrained(
            LOCAL_MODEL_PATH, num_labels=3, ignore_mismatched_sizes=True
        )
    else:
        tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)
        model = BertForSequenceClassification.from_pretrained(
            config.MODEL_NAME, num_labels=3
        )

    model.to(device)

    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, config.MAX_LEN)
    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, config.MAX_LEN)

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        num_workers=config.NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )

    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)

    # -------------------------------------------------
    # â˜… æ–°å¢ï¼šç±»åˆ«æƒé‡ï¼ˆåªæ”¹è¿™é‡Œï¼‰
    # ç±»åˆ« 2 æƒé‡å–ä¸­â€œ2.5â€ï¼Œç¼“è§£ä¸å¹³è¡¡
    # -------------------------------------------------
    class_weights = torch.tensor([1.0, 1.0, 2.5], device=device)
    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

    epoch_losses = []

    for epoch in range(config.EPOCHS):
        avg_loss = train_model(
            model,
            train_loader,
            optimizer,
            device,
            use_amp,
            scaler,
            epoch,
            loss_fn
        )
        print(f"Epoch {epoch + 1} - Loss: {avg_loss:.4f}")
        epoch_losses.append(avg_loss)

    evaluate_model(model, val_loader, device)

    os.makedirs(config.MODEL_SAVE_PATH, exist_ok=True)
    model.save_pretrained(config.MODEL_SAVE_PATH)
    tokenizer.save_pretrained(config.MODEL_SAVE_PATH)

    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {config.MODEL_SAVE_PATH}")

    if SAVE_TRAIN_LOG:
        log_path = os.path.join(config.MODEL_SAVE_PATH, "training_log.csv")
        save_training_log(log_path, epoch_losses)


# -------------------------------------------------
# å…¥å£
# -------------------------------------------------
if __name__ == "__main__":
    main()
"""
D:\AnacondaLocation\envs\nlp\python.exe D:\æ¯•è®¾\ä»£ç \sentiment_analysis\train_sentiment_v3.py 
ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š
Original Sentence    2
Sentiment            3
dtype: int64
ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼šD:\æ¯•è®¾\ä»£ç \models\bert-base-chinese
Epoch 2:   0%|          | 0/500 [00:00<?, ?it/s]Epoch 1 - Loss: 0.9239
Epoch 3:   0%|          | 0/500 [00:00<?, ?it/s]Epoch 2 - Loss: 0.7212
Epoch 3 - Loss: 0.5283
              precision    recall  f1-score   support

           0     0.5818    0.8324    0.6849       346
           1     0.7829    0.5469    0.6440       554
           2     0.4274    0.5051    0.4630        99

    accuracy                         0.6416       999
   macro avg     0.5974    0.6281    0.5973       999
weighted avg     0.6780    0.6416    0.6402       999

æ¨¡å‹å·²ä¿å­˜åˆ° sentiment_analysis\sentiment_bert_model_v3

è¿›ç¨‹å·²ç»“æŸï¼Œé€€å‡ºä»£ç ä¸º 0

"""

"""é€‰æ‹©å“ªä¸€ä¸ªç‰ˆæœ¬ï¼Ÿ"""
"""
v1 / v2 / v3 æƒ…æ„Ÿæ¨¡å‹å®éªŒç»“æœå¯¹æ¯”
è¡¨ 1 ä¸åŒæ¨¡å‹ç‰ˆæœ¬åœ¨éªŒè¯é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½å¯¹æ¯”
ç‰ˆæœ¬	              ç±»åˆ«2æƒé‡	    Accuracy	Macro_F1	Weighted_F1	  ç±»åˆ«2_Precision	 ç±»åˆ«2_Recall	   ç±»åˆ«2_F1
v1.2ï¼ˆæœ€æ–°ä¸€æ¬¡ï¼‰	      æ— 	     0.6927	    0.6154	     0.6878	          0.5574	          0.3434	    0.4250
v2	                 3.0	     0.6537	    0.5935	     0.6575	          0.3706	          0.5354	    0.4380
v3	                 2.5	     0.6416	    0.5973	     0.6402	          0.4274	          0.5051	    0.4630

è¯´æ˜ï¼š
ç±»åˆ« 0 / 1 / 2 åˆ†åˆ«å¯¹åº”ï¼šè´Ÿå‘ / ä¸­æ€§ / æ­£å‘ï¼ˆå¼ºçƒˆè¯„ä»·ï¼‰
ç±»åˆ« 2 ä¸ºæ ·æœ¬æ•°é‡æœ€å°‘çš„å°‘æ•°ç±»
v1 æœ€æ–°ç»“æœæ¥è‡ªä½ æœ€åä¸€æ¬¡é‡æ–°è®­ç»ƒçš„è¾“å‡º

äºŒã€å®éªŒç°è±¡çš„å®¢è§‚è§£è¯»ï¼ˆä¸ä¸‹ç»“è®ºï¼‰
1ï¸âƒ£ v1ï¼ˆæ— ç±»åˆ«æƒé‡ï¼‰
æ•´ä½“ Accuracyã€Weighted F1 æœ€é«˜
ä¸»æµç±»åˆ«ï¼ˆ0ã€1ï¼‰é¢„æµ‹ç¨³å®š
ç±»åˆ« 2ï¼š
Precision è¾ƒé«˜
Recall æ˜æ˜¾åä½ï¼ˆæ¼åˆ¤è¾ƒå¤šï¼‰
è¯´æ˜ï¼šæ¨¡å‹æ›´å€¾å‘äºâ€œè°¨æ…â€é¢„æµ‹ç±»åˆ« 2ï¼Œåªåœ¨æŠŠæ¡è¾ƒé«˜æ—¶æ‰ç»™å‡ºè¯¥æ ‡ç­¾ã€‚
2ï¸âƒ£ v2ï¼ˆç±»åˆ« 2 æƒé‡ = 3ï¼‰
ç±»åˆ« 2 Recall æ˜¾è‘—æå‡
ä½† Precision æ˜æ˜¾ä¸‹é™
æ•´ä½“ Accuracy ä¸ Weighted F1 å‡ä¸‹é™
è¯´æ˜ï¼šæƒé‡è¿‡å¤§å¯¼è‡´æ¨¡å‹å¯¹ç±»åˆ« 2 è¿‡åº¦æ•æ„Ÿï¼Œå‡ºç°ä¸€å®šç¨‹åº¦çš„â€œæ³›åŒ–æ‰©å¼ â€ã€‚
3ï¸âƒ£ v3ï¼ˆç±»åˆ« 2 æƒé‡ = 2.5ï¼‰
ç±»åˆ« 2 çš„ Precision / Recall è¾¾åˆ°ç›¸å¯¹å‡è¡¡
ç±»åˆ« 2 F1 ä¸ºä¸‰è€…ä¸­æœ€é«˜
ä½†æ•´ä½“æ€§èƒ½è¿›ä¸€æ­¥ä¸‹é™
è¯´æ˜ï¼šv3 æ˜¯ä¸€ä¸ª**â€œå°‘æ•°ç±»å‹å¥½å‹â€æ¨¡å‹**ï¼Œä½†ä»¥ç‰ºç‰²æ•´ä½“ç¨³å®šæ€§ä¸ºä»£ä»·ã€‚
"""



"""
1,è‡ªåŠ¨æ£€æµ‹ GPU/CPUï¼š
GPU å¯ç”¨ â†’ AMP è‡ªåŠ¨å¼€å¯ + num_workers å¢åŠ  + pin_memory=True
GPU ä¸å¯ç”¨ â†’ AMP è‡ªåŠ¨å…³é—­ï¼ŒCPU è®­ç»ƒæ­£å¸¸

2,AMP ä½¿ç”¨ï¼š
åªæœ‰ GPU æœ‰æ•ˆï¼Œä¸ä¼šå†å‡ºç° CPU è­¦å‘Š
åœ¨ GPU ä¸Šè®­ç»ƒé€Ÿåº¦æå‡ 1.5~2 å€ï¼Œæ˜¾å­˜å ç”¨é™ä½

3,DataLoader ä¼˜åŒ–ï¼š
GPUï¼šæ›´å¤šçº¿ç¨‹ + pinned memoryï¼Œæé«˜åŠ è½½é€Ÿåº¦
CPUï¼šçº¿ç¨‹é€‚ä¸­ï¼Œé˜²æ­¢ç³»ç»Ÿå¡é¡¿

4,åŸæ³¨é‡Šã€åŠŸèƒ½ä¿æŒä¸å˜ï¼š
è®­ç»ƒæ—¥å¿—ã€è¯„ä¼°ã€æ¨¡å‹ä¿å­˜ç­‰éƒ½ä¸å—å½±å“
tqdm æ˜¾ç¤ºä»ç„¶æ¸…æ™°ã€æ¯ epoch å†…æ˜¾ç¤ºè¿›åº¦
"""