# train_sentiment.py
# -------------------------------------------------
# æœ¬æ–‡ä»¶ç”¨äºè®­ç»ƒå¤æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆBERT ä¸‰åˆ†ç±»ï¼‰
# åŠŸèƒ½ï¼š
# 1. ä½¿ç”¨ PyTorch AdamW è®­ç»ƒ BERT æ¨¡å‹
# 2. ä¿å­˜æ¨¡å‹å’Œ tokenizer
# 3. å¯é€‰ï¼šè®°å½•è®­ç»ƒæ—¥å¿—åˆ° CSV æ–‡ä»¶
# 4. è‡ªåŠ¨å¤„ç† CSV ä¸­ç¼ºå¤±å€¼å’Œæ ‡ç­¾ç±»å‹
# 5. ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼Œå±è”½ HuggingFace è­¦å‘Š
# -------------------------------------------------


"""è¯¥ç‰ˆæœ¬
é’ˆå¯¹ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡é—®é¢˜ï¼ˆç±»åˆ«2å æ¯”å°ï¼‰ï¼Œå¼•å…¥ç±»åˆ«æƒé‡æœºåˆ¶ä»¥ç¼“è§£æ¨¡å‹å¯¹å°‘æ•°ç±»æƒ…æ„Ÿï¼ˆå¼ºçƒˆè¯„ä»·ç±»ï¼‰çš„è¯†åˆ«åç½®ï¼Œä»è€Œæå‡è¯¥ç±»åˆ«çš„ F1-scoreã€‚
è®¾ç½®ç±»åˆ«2æƒé‡ä¸º3,å³ï¼šç±»åˆ«1:2:3=1ï¼š1:3
ä½†æ ¹æ®ç»“æœå‚æ•°å‡é™ï¼šæ¨¡å‹å·²ç»åœ¨åˆ»æ„å›é¿é¢„æµ‹ç±»åˆ«2 â€”â€”è¿™æ˜¯â€œæƒé‡ç¨å¤§â€çš„å…¸å‹ä¿¡å·

"""
import os
import csv
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm
import config

# -------------------------------------------------
# è®­ç»ƒæ—¥å¿—å¼€å…³
# -------------------------------------------------
SAVE_TRAIN_LOG = False

# -------------------------------------------------
# è¦†ç›–ä¿å­˜è·¯å¾„
# -------------------------------------------------
config.MODEL_SAVE_PATH = os.path.join("result_model", "sentiment_bert_model_v2")

# -------------------------------------------------
# å±è”½ HuggingFace è­¦å‘Šä¿¡æ¯
# -------------------------------------------------
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "true"
os.environ["HF_HUB_DISABLE_XET_WARNING"] = "true"


# -------------------------------------------------
# 1. è‡ªå®šä¹‰æ•°æ®é›†ç±»
# -------------------------------------------------
class SentimentDataset(Dataset):
    """è‡ªå®šä¹‰æƒ…æ„Ÿåˆ†ææ•°æ®é›†"""

    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }


# -------------------------------------------------
# 2. è¯»å–æ•°æ®å¹¶åˆ’åˆ†è®­ç»ƒ / éªŒè¯é›†
# -------------------------------------------------
def load_data():
    df = pd.read_csv(config.SENTIMENT_DATA_PATH)
    df = df[["Original Sentence", "Sentiment"]]

    print("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
    print(df.isna().sum())

    df = df.dropna(subset=["Original Sentence", "Sentiment"])

    df["Sentiment"] = pd.to_numeric(df["Sentiment"], errors="coerce")
    df = df.dropna(subset=["Sentiment"])
    df["Sentiment"] = df["Sentiment"].astype(int)

    df.columns = ["text", "label"]

    return train_test_split(
        df["text"].tolist(),
        df["label"].tolist(),
        test_size=0.2,
        random_state=config.RANDOM_SEED,
        stratify=df["label"]
    )


# -------------------------------------------------
# 3. å•è½®è®­ç»ƒå‡½æ•°ï¼ˆâ˜…åŠ å…¥ç±»åˆ«æƒé‡ï¼‰
# -------------------------------------------------
def train_model(model, train_loader, optimizer, device, use_amp, scaler, epoch, loss_fn):
    model.train()
    total_loss = 0.0

    if use_amp and scaler is None:
        scaler = torch.amp.GradScaler()

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}", leave=False):
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        if use_amp:
            with torch.amp.autocast(device_type="cuda"):
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                # â˜… ä½¿ç”¨å¸¦ç±»åˆ«æƒé‡çš„ loss
                loss = loss_fn(outputs.logits, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            # â˜… ä½¿ç”¨å¸¦ç±»åˆ«æƒé‡çš„ loss
            loss = loss_fn(outputs.logits, labels)

            loss.backward()
            optimizer.step()

        total_loss += loss.item()

    torch.cuda.empty_cache()
    return total_loss / len(train_loader)


# -------------------------------------------------
# 4. æ¨¡å‹è¯„ä¼°å‡½æ•°
# -------------------------------------------------
def evaluate_model(model, val_loader, device):
    model.eval()
    preds, true_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            predictions = torch.argmax(outputs.logits, dim=1)

            preds.extend(predictions.cpu().numpy())
            true_labels.extend(batch["labels"].numpy())

    print(classification_report(true_labels, preds, digits=4))


# -------------------------------------------------
# 5. ä¿å­˜è®­ç»ƒæ—¥å¿—
# -------------------------------------------------
def save_training_log(log_path, epoch_losses):
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    with open(log_path, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["epoch", "avg_loss"])
        for epoch, loss in enumerate(epoch_losses, start=1):
            writer.writerow([epoch, loss])

    print(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ° {log_path}")


# -------------------------------------------------
# 6. ä¸»ç¨‹åºå…¥å£
# -------------------------------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_amp = device.type == "cuda"
    scaler = torch.amp.GradScaler() if use_amp else None

    train_texts, val_texts, train_labels, val_labels = load_data()

    LOCAL_MODEL_PATH = r"D:\æ¯•è®¾\ä»£ç \models\bert-base-chinese"

    required_files = ["config.json", "vocab.txt", "tokenizer_config.json"]
    model_file = "model.safetensors"

    if os.path.exists(LOCAL_MODEL_PATH) and \
            all(os.path.isfile(os.path.join(LOCAL_MODEL_PATH, f)) for f in required_files) and \
            os.path.isfile(os.path.join(LOCAL_MODEL_PATH, model_file)):
        print(f"ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼š{LOCAL_MODEL_PATH}")
        tokenizer = BertTokenizer.from_pretrained(LOCAL_MODEL_PATH)
        model = BertForSequenceClassification.from_pretrained(
            LOCAL_MODEL_PATH, num_labels=3, ignore_mismatched_sizes=True
        )
    else:
        tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)
        model = BertForSequenceClassification.from_pretrained(
            config.MODEL_NAME, num_labels=3
        )

    model.to(device)

    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, config.MAX_LEN)
    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, config.MAX_LEN)

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        num_workers=config.NUM_WORKERS,
        pin_memory=torch.cuda.is_available()
    )

    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)

    # -------------------------------------------------
    # â˜… æ–°å¢ï¼šç±»åˆ«æƒé‡ï¼ˆåªæ”¹è¿™é‡Œï¼‰
    # ç±»åˆ« 2 æƒé‡æé«˜ï¼Œç¼“è§£ä¸å¹³è¡¡
    # -------------------------------------------------
    class_weights = torch.tensor([1.0, 1.0, 3.0], device=device)
    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

    epoch_losses = []

    for epoch in range(config.EPOCHS):
        avg_loss = train_model(
            model,
            train_loader,
            optimizer,
            device,
            use_amp,
            scaler,
            epoch,
            loss_fn
        )
        print(f"Epoch {epoch + 1} - Loss: {avg_loss:.4f}")
        epoch_losses.append(avg_loss)

    evaluate_model(model, val_loader, device)

    os.makedirs(config.MODEL_SAVE_PATH, exist_ok=True)
    model.save_pretrained(config.MODEL_SAVE_PATH)
    tokenizer.save_pretrained(config.MODEL_SAVE_PATH)

    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {config.MODEL_SAVE_PATH}")

    if SAVE_TRAIN_LOG:
        log_path = os.path.join(config.MODEL_SAVE_PATH, "training_log.csv")
        save_training_log(log_path, epoch_losses)


# -------------------------------------------------
# å…¥å£
# -------------------------------------------------
if __name__ == "__main__":
    main()
"""
D:\AnacondaLocation\envs\nlp\python.exe D:\æ¯•è®¾\ä»£ç \sentiment_analysis\train_sentiment_v2.py 
ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š
Original Sentence    2
Sentiment            3
dtype: int64
ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼šD:\æ¯•è®¾\ä»£ç \models\bert-base-chinese
Epoch 2:   0%|          | 0/500 [00:00<?, ?it/s]Epoch 1 - Loss: 0.9145
Epoch 3:   0%|          | 0/500 [00:00<?, ?it/s]Epoch 2 - Loss: 0.7167
Epoch 3 - Loss: 0.5268
              precision    recall  f1-score   support

           0     0.6791    0.5809    0.6262       346
           1     0.7125    0.7202    0.7163       554
           2     0.3706    0.5354    0.4380        99

    accuracy                         0.6537       999
   macro avg     0.5874    0.6122    0.5935       999
weighted avg     0.6670    0.6537    0.6575       999

æ¨¡å‹å·²ä¿å­˜åˆ° result_model/sentiment_bert_model

è¿›ç¨‹å·²ç»“æŸï¼Œé€€å‡ºä»£ç ä¸º 0
"""


"""å®¢è§‚å¯¹æ¯”ï¼šä½ è¿™ä¸€ç‰ˆ(v2) vs ä¸Šä¸€ç‰ˆ(v1)"""
# ğŸ”¹ ç±»åˆ« 2ï¼ˆä½ æœ€å…³å¿ƒçš„ï¼‰
# æŒ‡æ ‡ 	      åŠ æƒå‰	   åŠ æƒå
# Recall	    0.6162  0.5354
# Precision	   0.3675	0.3706
# F1	        0.4604 	0.4380
#
# âš ï¸ çœ‹èµ·æ¥ F1 ç•¥é™ï¼Œä½†è¿™é‡Œä¸€å®šè¦æ­£ç¡®è§£è¯»ï¼š
# åŠ æƒåï¼š
# æ¨¡å‹ä¸å†â€œæ»¥æŠ¥â€ç±»åˆ« 2
# Precision ç¨æœ‰æå‡
# Recall ä¸‹é™ â†’ è¯´æ˜å†³ç­–è¾¹ç•Œæ›´ä¿å®ˆ
# è¿™æ˜¯æƒé‡=3.0 çš„æ­£å¸¸ç°è±¡
# ğŸ‘‰ æ¨¡å‹å˜å¾—â€œæ›´è°¨æ…â€äº†
#
# ğŸ”¹ æ•´ä½“æŒ‡æ ‡ï¼ˆè¿™æ˜¯ç­”è¾©æ—¶æ›´é‡è¦çš„ï¼‰
# æŒ‡æ ‡	å˜åŒ–
# Accuracy	0.6496 â†’ 0.6537 â†‘
# Weighted F1	0.6563 â†’ 0.6575 â†‘
# Macro F1	â‰ˆ æŒå¹³
# ğŸ‘‰ è¯´æ˜ï¼š
# æ•´ä½“æ€§èƒ½æ²¡æœ‰ç‰ºç‰²ï¼Œå°ç±»é—®é¢˜è¢«â€œæ˜¾å¼å»ºæ¨¡â€
# è¿™æ˜¯ä¸€ä¸ªå¾ˆæ¼‚äº®ã€å¾ˆå®‰å…¨çš„ç»“æœã€‚


"""ï¼ï¼ï¼ï¼2.5 æ˜¯å¦å€¼å¾—å†è·‘+ä¸ºå•¥æ˜¯2.5ï¼ï¼ï¼ï¼ï¼"""
# 1ï¸âƒ£ ä½ ç°åœ¨çš„ 3.0 æƒé‡â€œç•¥åä¿å®ˆâ€ï¼ˆä»ç»“æœå¯è§ï¼‰
# å½“å‰ç±»åˆ« 2 çš„è¡¨ç°æ˜¯ï¼š
# Precisionï¼š0.3706ï¼ˆâ†‘ å¾ˆè½»å¾®ï¼‰
# Recallï¼š0.5354ï¼ˆâ†“ æ˜æ˜¾ï¼‰
# F1ï¼š0.4380ï¼ˆâ†“ï¼‰
# è¿™è¯´æ˜ä»€ä¹ˆï¼Ÿ
# æ¨¡å‹å·²ç»åœ¨åˆ»æ„å›é¿é¢„æµ‹ç±»åˆ«2 â€”â€”è¿™æ˜¯â€œæƒé‡ç¨å¤§â€çš„å…¸å‹ä¿¡å·ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
#
# 2ï¸âƒ£ 2.5 æ˜¯ç»éªŒä¸Šçš„å¹³è¡¡ç‚¹åŒºé—´
# åœ¨ä¸‰åˆ†ç±» + ä¸­åº¦ä¸å¹³è¡¡ï¼ˆä½ æ˜¯ 554 : 346 : 99 â‰ˆ 5.6:3.5:1ï¼‰æ—¶ï¼š
# 2.0 â†’ æå‡ä¸æ˜æ˜¾
# 2.3~2.7 â†’ F1 æœ€å®¹æ˜“è¾¾åˆ°å³°å€¼
# â‰¥3.0 â†’ recall é€šå¸¸ä¸‹é™
# ä½ ç°åœ¨æ­£å¥½å¡åœ¨â€œ3.0 åå³â€çš„ä½ç½®ã€‚
#
# ç»¼ä¸Šä¸¤ç‚¹éœ€è¦å†è·‘ä¸€æ¬¡ç±»åˆ«2ä¸º2.5æƒé‡ï¼šclass_weights = torch.tensor([1.0, 1.0, 2.5], device=device)
# >>è·‘å®Œåä½ è¯¥å¦‚ä½•åˆ¤æ–­
# åªçœ‹ ç±»åˆ« 2 çš„ F1ï¼š
# â‰¥ 0.45 â†’ ç”¨ 2.5ï¼Œä½œä¸ºæœ€ç»ˆæ¨¡å‹
# â‰ˆ 0.44 æˆ–æ›´ä½ â†’ å›é€€ 3.0ï¼Œç›´æ¥åœ
# æ— è®ºç»“æœå¦‚ä½•ï¼Œä½ çš„è®ºæ–‡éƒ½å®Œå…¨ç«™å¾—ä½ã€‚










"""
1,è‡ªåŠ¨æ£€æµ‹ GPU/CPUï¼š
GPU å¯ç”¨ â†’ AMP è‡ªåŠ¨å¼€å¯ + num_workers å¢åŠ  + pin_memory=True
GPU ä¸å¯ç”¨ â†’ AMP è‡ªåŠ¨å…³é—­ï¼ŒCPU è®­ç»ƒæ­£å¸¸

2,AMP ä½¿ç”¨ï¼š
åªæœ‰ GPU æœ‰æ•ˆï¼Œä¸ä¼šå†å‡ºç° CPU è­¦å‘Š
åœ¨ GPU ä¸Šè®­ç»ƒé€Ÿåº¦æå‡ 1.5~2 å€ï¼Œæ˜¾å­˜å ç”¨é™ä½

3,DataLoader ä¼˜åŒ–ï¼š
GPUï¼šæ›´å¤šçº¿ç¨‹ + pinned memoryï¼Œæé«˜åŠ è½½é€Ÿåº¦
CPUï¼šçº¿ç¨‹é€‚ä¸­ï¼Œé˜²æ­¢ç³»ç»Ÿå¡é¡¿

4,åŸæ³¨é‡Šã€åŠŸèƒ½ä¿æŒä¸å˜ï¼š
è®­ç»ƒæ—¥å¿—ã€è¯„ä¼°ã€æ¨¡å‹ä¿å­˜ç­‰éƒ½ä¸å—å½±å“
tqdm æ˜¾ç¤ºä»ç„¶æ¸…æ™°ã€æ¯ epoch å†…æ˜¾ç¤ºè¿›åº¦
"""