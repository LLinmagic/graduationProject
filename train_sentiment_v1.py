# train_sentiment.py
# -------------------------------------------------
# æœ¬æ–‡ä»¶ç”¨äºè®­ç»ƒå¤æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆBERT ä¸‰åˆ†ç±»ï¼‰
# åŠŸèƒ½ï¼š
# 1. ä½¿ç”¨ PyTorch AdamW è®­ç»ƒ BERT æ¨¡å‹
# 2. ä¿å­˜æ¨¡å‹å’Œ tokenizer
# 3. å¯é€‰ï¼šè®°å½•è®­ç»ƒæ—¥å¿—åˆ° CSV æ–‡ä»¶
# 4. è‡ªåŠ¨å¤„ç† CSV ä¸­ç¼ºå¤±å€¼å’Œæ ‡ç­¾ç±»å‹
# 5. ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼Œå±è”½ HuggingFace è­¦å‘Š
# -------------------------------------------------
"""
æ ¹æ®v1ã€v2ã€v3ç‰ˆæœ¬çš„å¯¹æ¯”åˆ†æï¼Œæœ€ç»ˆé€‰æ‹©v1ç‰ˆæœ¬ä¸ºæœ€ç»ˆç‰ˆæœ¬
"""

"""è¯¥ç‰ˆæœ¬
é€‰å– 95% åˆ†ä½æ•°ï¼ˆ41 tokensï¼‰å¹¶ç•™æœ‰å†—ä½™ï¼Œå°†æœ€å¤§åºåˆ—é•¿åº¦è®¾ä¸º 48ï¼Œä»¥åœ¨ä¿è¯è¯­ä¹‰å®Œæ•´æ€§çš„åŒæ—¶é™ä½è®¡ç®—å¤æ‚åº¦ã€‚
ç±»åˆ« 2 è¡¨ç°å·®ï¼ˆä¸æ˜¯ä»£ç çš„é—®é¢˜ï¼‰ï¼Œä¸»è¦æ˜¯ç±»åˆ«æåº¦ä¸å¹³è¡¡ï¼š
0: 346
1: 554
2:  99  â† ä¸¥é‡å°‘
BERT åœ¨è®­ç»ƒæ—¶ä¼šè‡ªç„¶åå‘ 0 / 1ã€‚
"""
import os
import csv
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm
import config

# -------------------------------------------------
# è®­ç»ƒæ—¥å¿—å¼€å…³
# Trueï¼šä¿å­˜è®­ç»ƒæ—¥å¿— CSV
# Falseï¼šä¸ä¿å­˜
# -------------------------------------------------
SAVE_TRAIN_LOG = False

# -------------------------------------------------
# å±è”½ HuggingFace è­¦å‘Šä¿¡æ¯ï¼ˆsymlink å’Œ Xet Storageï¼‰
# ä»…å½±å“æ—¥å¿—è¾“å‡ºï¼Œä¸å½±å“æ¨¡å‹åŠŸèƒ½
# -------------------------------------------------
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "true"
os.environ["HF_HUB_DISABLE_XET_WARNING"] = "true"

# -------------------------------------------------
# 1. è‡ªå®šä¹‰æ•°æ®é›†ç±»
# å°†æ–‡æœ¬å’Œæ ‡ç­¾å°è£…æˆ PyTorch Dataset
# BERT éœ€è¦ input_idsã€attention_mask
# -------------------------------------------------
class SentimentDataset(Dataset):
    """è‡ªå®šä¹‰æƒ…æ„Ÿåˆ†ææ•°æ®é›†"""
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # ä½¿ç”¨ tokenizer å°†æ–‡æœ¬ç¼–ç ä¸º BERT å¯æ¥å—çš„æ ¼å¼
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

# -------------------------------------------------
# 2. è¯»å–æ•°æ®å¹¶åˆ’åˆ†è®­ç»ƒ / éªŒè¯é›†
# -------------------------------------------------
def load_data():
    """
    è¯»å– CSV æ•°æ®ï¼Œæ£€æŸ¥ç¼ºå¤±å€¼å’Œæ ‡ç­¾ç±»å‹ï¼Œ
    åˆ é™¤ç¼ºå¤±æ–‡æœ¬æˆ–æ ‡ç­¾è¡Œï¼Œå°†æ ‡ç­¾è½¬ä¸ºæ•´æ•°
    è¿”å›ï¼šè®­ç»ƒé›†æ–‡æœ¬ã€éªŒè¯é›†æ–‡æœ¬ã€è®­ç»ƒé›†æ ‡ç­¾ã€éªŒè¯é›†æ ‡ç­¾
    """
    df = pd.read_csv(config.SENTIMENT_DATA_PATH)
    df = df[["Original Sentence", "Sentiment"]]

    # æ‰“å°ç¼ºå¤±å€¼ç»Ÿè®¡
    print("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
    print(df.isna().sum())

    # åˆ é™¤ç¼ºå¤±å€¼
    df = df.dropna(subset=["Original Sentence", "Sentiment"])

    # å°†æ ‡ç­¾è½¬ä¸ºæ•´æ•°ï¼Œæ— æ³•è½¬æ¢çš„ä¼šè¢«åˆ é™¤
    df["Sentiment"] = pd.to_numeric(df["Sentiment"], errors="coerce")
    df = df.dropna(subset=["Sentiment"])
    df["Sentiment"] = df["Sentiment"].astype(int)

    df.columns = ["text", "label"]

    # ä½¿ç”¨ stratify ä¿æŒå„ç±»åˆ«æ¯”ä¾‹
    return train_test_split(
        df["text"].tolist(),
        df["label"].tolist(),
        test_size=0.2,
        random_state=config.RANDOM_SEED,
        stratify=df["label"]
    )

# -------------------------------------------------
# 3. å•è½®è®­ç»ƒå‡½æ•°ï¼ˆä¼˜åŒ– tqdm è¾“å‡º + AMP è‡ªåŠ¨åˆ‡æ¢ï¼‰
# -------------------------------------------------
def train_model(model, train_loader, optimizer, device, use_amp, scaler, epoch):
    """
    å¯¹æ¨¡å‹è¿›è¡Œä¸€ä¸ª epoch çš„è®­ç»ƒ
    è¿”å›å¹³å‡ loss
    tqdm æ˜¾ç¤º epoch å†…æ¯ä¸ª batch çš„è¿›åº¦
    """
    model.train()
    total_loss = 0

    # GPU ä¸Šå¯ç”¨ AMPï¼ŒCPU ä¸å¯ç”¨
    if use_amp and scaler is None:
        scaler = torch.amp.GradScaler()

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}", leave=False):
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        if use_amp:
            # GPU ä½¿ç”¨ AMP
            with torch.amp.autocast(device_type='cuda'):
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            # CPU æ™®é€šè®­ç»ƒ
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            loss.backward()
            optimizer.step()

        total_loss += loss.item()

    torch.cuda.empty_cache()
    return total_loss / len(train_loader)

# -------------------------------------------------
# 4. æ¨¡å‹è¯„ä¼°å‡½æ•°
# -------------------------------------------------
def evaluate_model(model, val_loader, device):
    """
    å¯¹éªŒè¯é›†è¿›è¡Œè¯„ä¼°ï¼Œè¾“å‡ºåˆ†ç±»æŠ¥å‘Šï¼ˆprecision, recall, f1ï¼‰
    """
    model.eval()
    preds, true_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            predictions = torch.argmax(outputs.logits, dim=1)
            preds.extend(predictions.cpu().numpy())
            true_labels.extend(batch["labels"].numpy())

    print(classification_report(true_labels, preds, digits=4))

# -------------------------------------------------
# 5. ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ° CSV
# -------------------------------------------------
def save_training_log(log_path, epoch_losses):
    """
    å°†æ¯ä¸ª epoch çš„å¹³å‡ loss å†™å…¥ CSV æ–‡ä»¶
    """
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    with open(log_path, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['epoch', 'avg_loss'])
        for epoch, loss in enumerate(epoch_losses, start=1):
            writer.writerow([epoch, loss])
    print(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ° {log_path}")

# -------------------------------------------------
# 6. ä¸»ç¨‹åºå…¥å£
# -------------------------------------------------
def main():
    # é€‰æ‹©è®¾å¤‡ï¼ˆGPU ä¼˜å…ˆï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_amp = device.type == "cuda"

    # GPU ä¸Šå¯ç”¨ AMP scalerï¼ŒCPU ä¸å¯ç”¨
    scaler = torch.amp.GradScaler() if use_amp else None

    # è¯»å–æ•°æ®
    train_texts, val_texts, train_labels, val_labels = load_data()

    # æŒ‡å®šæœ¬åœ°ç¼“å­˜æ¨¡å‹è·¯å¾„:ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼ˆé¿å…ç½‘ç»œ ReadTimeoutï¼‰
    LOCAL_MODEL_PATH = r"D:\æ¯•è®¾\ä»£ç \models\bert-base-chinese"

    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜å®Œæ•´æ€§
    required_files = ["config.json", "vocab.txt", "tokenizer_config.json"]
    model_file = "model.safetensors"

    if os.path.exists(LOCAL_MODEL_PATH) and \
            all(os.path.isfile(os.path.join(LOCAL_MODEL_PATH, f)) for f in required_files) and \
            os.path.isfile(os.path.join(LOCAL_MODEL_PATH, model_file)):
        print(f"ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼š{LOCAL_MODEL_PATH}")
        tokenizer = BertTokenizer.from_pretrained(LOCAL_MODEL_PATH)
        model = BertForSequenceClassification.from_pretrained(
            LOCAL_MODEL_PATH, num_labels=3, ignore_mismatched_sizes=True
        )
    else:
        print("æœ¬åœ°ç¼“å­˜ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨ï¼Œå°è¯•ä»ç½‘ç»œä¸‹è½½æ¨¡å‹")
        tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)
        model = BertForSequenceClassification.from_pretrained(
            config.MODEL_NAME, num_labels=3
        )

    model.to(device)

    # æ„å»ºæ•°æ®é›†å’Œ DataLoader
    num_workers = config.NUM_WORKERS
    pin_memory = True if torch.cuda.is_available() else False

    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, config.MAX_LEN)
    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, config.MAX_LEN)

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        num_workers=num_workers,
        pin_memory=pin_memory
    )

    # ä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)

    # è®°å½•æ¯ä¸ª epoch å¹³å‡ loss
    epoch_losses = []

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.EPOCHS):
        avg_loss = train_model(model, train_loader, optimizer, device, use_amp, scaler, epoch)
        print(f"Epoch {epoch + 1} - Loss: {avg_loss:.4f}")
        epoch_losses.append(avg_loss)

    # éªŒè¯
    evaluate_model(model, val_loader, device)

    # ä¿å­˜æ¨¡å‹å’Œ tokenizer
    os.makedirs(config.MODEL_SAVE_PATH, exist_ok=True)
    model.save_pretrained(config.MODEL_SAVE_PATH)
    tokenizer.save_pretrained(config.MODEL_SAVE_PATH)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {config.MODEL_SAVE_PATH}")

    # æ ¹æ®å¼€å…³ä¿å­˜è®­ç»ƒæ—¥å¿—
    if SAVE_TRAIN_LOG:
        log_path = os.path.join(config.MODEL_SAVE_PATH, "training_log.csv")
        save_training_log(log_path, epoch_losses)

# -------------------------------------------------
# å…¥å£
# -------------------------------------------------
if __name__ == "__main__":
    main()


"""
sentiment_bert_model_v1 ä¸å°å¿ƒè¦†ç›–äº†åˆ æ‰äº†
è¾“å‡ºäº†ï¼šD:\AnacondaLocation\envs\nlp\python.exe D:\æ¯•è®¾\ä»£ç \sentiment_analysis\train_sentiment.py 
ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š 
Original Sentence 2
Sentiment 3 
dtype: int64 
ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼šD:\æ¯•è®¾\ä»£ç \models\bert-base-chinese 
Epoch 2: 0%| | 0/500 [00:00<?, ?it/s]Epoch 1 - 
Loss: 0.8105 
Epoch 3: 0%| | 0/500 [00:00<?, ?it/s]Epoch 2 - 
Loss: 0.6322 
Epoch 3 - Loss: 0.4431 
 precision recall f1-score support 
0 0.6877   0.5983   0.6399   346 
1 0.7162   0.6877   0.7017   554 
2 0.3675   0.6162   0.4604   99
 accuracy 0.6496 999 
 macro avg 0.5904 0.6341 0.6006 999 
 weighted avg 0.6718 0.6496 0.6563 999 
 æ¨¡å‹å·²ä¿å­˜åˆ°result_model/sentiment_bert_model è¿›ç¨‹å·²ç»“æŸï¼Œé€€å‡ºä»£ç ä¸º 0
 
 
 ç¬¬äºŒæ¬¡è¿è¡Œï¼š
 D:\AnacondaLocation\envs\nlp\python.exe D:\æ¯•è®¾\ä»£ç \sentiment_analysis\train_sentiment_v1.py 
ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š
Original Sentence    2
Sentiment            3
dtype: int64
ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹ï¼šD:\æ¯•è®¾\ä»£ç \models\bert-base-chinese
Epoch 2:   0%|          | 0/500 [00:00<?, ?it/s]Epoch 1 - Loss: 0.8208
Epoch 3:   0%|          | 0/500 [00:00<?, ?it/s]Epoch 2 - Loss: 0.6326
Epoch 3 - Loss: 0.4587
              precision    recall  f1-score   support

           0     0.6494    0.7225    0.6840       346
           1     0.7378    0.7365    0.7371       554
           2     0.5574    0.3434    0.4250        99

    accuracy                         0.6927       999
   macro avg     0.6482    0.6008    0.6154       999
weighted avg     0.6893    0.6927    0.6878       999

æ¨¡å‹å·²ä¿å­˜åˆ° result_model/sentiment_bert_model_v1
è¿›ç¨‹å·²ç»“æŸï¼Œé€€å‡ºä»£ç ä¸º 0

ğŸ“Œ æ—§ v1ï¼ˆä½ ä¹‹å‰ï¼‰
ç±»åˆ«	Precision	Recall	F1
2	0.3675	0.6162	0.4604
ğŸ‘‰ å°‘æ•°ç±»ï¼š
åå¬å›ï¼ˆRecall é«˜ï¼‰
ç²¾åº¦ä½ï¼Œè¯¯æŠ¥å¤š

ğŸ“Œ æ–° v1ï¼ˆè¿™ä¸€æ¬¡ï¼‰
ç±»åˆ«	Precision	Recall	F1
2	0.5574	0.3434	0.4250
ğŸ‘‰ å°‘æ•°ç±»ï¼š
Precision æ˜æ˜¾æå‡
Recall é™ä½
F1 ç•¥é™ä½†ä»åœ¨åˆç†åŒºé—´

ğŸ“Œ æ•´ä½“æŒ‡æ ‡å˜åŒ–ï¼ˆé‡ç‚¹ï¼‰
æŒ‡æ ‡	æ—§ v1	æ–° v1
Accuracy	0.6496	0.6927
Weighted F1	0.6563	0.6878
æ•´ä½“æ¨¡å‹è´¨é‡æ˜¯ä¸Šå‡çš„

å››ã€ä¸ºä»€ä¹ˆè¿™åè€Œæ˜¯â€œå¥½äº‹â€ï¼Ÿ
ä½ ç°åœ¨å¯ä»¥åˆç†åœ°å†™ï¼š
åœ¨å¤šæ¬¡ç‹¬ç«‹è®­ç»ƒä¸­ï¼Œæ¨¡å‹æ•´ä½“æ€§èƒ½ä¿æŒç¨³å®šï¼Œä½†åœ¨å°‘æ•°ç±»ï¼ˆç±»åˆ« 2ï¼‰ä¸Šï¼ŒPrecision ä¸ Recall å­˜åœ¨ä¸€å®š trade-offï¼Œè¿™ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„éšæœºæ€§ä¸€è‡´ã€‚
è¿™æ˜¯éå¸¸æ ‡å‡†ã€éå¸¸å­¦æœ¯çš„è¡¨è¿°ã€‚

åœ¨è®ºæ–‡ä¸­è¿™æ ·è§£é‡Šï¼ˆå¯ç›´æ¥ç”¨ï¼‰ï¼š
ç”±äº BERT æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«éšæœºåˆå§‹åŒ–ä¸ Dropout ç­‰æœºåˆ¶ï¼Œå³ä½¿åœ¨ç›¸åŒå‚æ•°è®¾ç½®ä¸‹ï¼Œå¤šæ¬¡è®­ç»ƒç»“æœä»å¯èƒ½å­˜åœ¨è½»å¾®å·®å¼‚ã€‚æœ¬æ–‡é€‰å–å…¶ä¸­ä¸€æ¬¡æ”¶æ•›ç¨³å®šã€æ•´ä½“æ€§èƒ½è¾ƒä¼˜çš„æ¨¡å‹ä½œä¸ºåç»­å®éªŒçš„åŸºçº¿æ¨¡å‹ã€‚
"""




"""
1,è‡ªåŠ¨æ£€æµ‹ GPU/CPUï¼š
GPU å¯ç”¨ â†’ AMP è‡ªåŠ¨å¼€å¯ + num_workers å¢åŠ  + pin_memory=True
GPU ä¸å¯ç”¨ â†’ AMP è‡ªåŠ¨å…³é—­ï¼ŒCPU è®­ç»ƒæ­£å¸¸

2,AMP ä½¿ç”¨ï¼š
åªæœ‰ GPU æœ‰æ•ˆï¼Œä¸ä¼šå†å‡ºç° CPU è­¦å‘Š
åœ¨ GPU ä¸Šè®­ç»ƒé€Ÿåº¦æå‡ 1.5~2 å€ï¼Œæ˜¾å­˜å ç”¨é™ä½

3,DataLoader ä¼˜åŒ–ï¼š
GPUï¼šæ›´å¤šçº¿ç¨‹ + pinned memoryï¼Œæé«˜åŠ è½½é€Ÿåº¦
CPUï¼šçº¿ç¨‹é€‚ä¸­ï¼Œé˜²æ­¢ç³»ç»Ÿå¡é¡¿

4,åŸæ³¨é‡Šã€åŠŸèƒ½ä¿æŒä¸å˜ï¼š
è®­ç»ƒæ—¥å¿—ã€è¯„ä¼°ã€æ¨¡å‹ä¿å­˜ç­‰éƒ½ä¸å—å½±å“
tqdm æ˜¾ç¤ºä»ç„¶æ¸…æ™°ã€æ¯ epoch å†…æ˜¾ç¤ºè¿›åº¦
"""